{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "# !tensorboard --bind_all --logdir=my_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import psutil\n",
    "\n",
    "# Getting % usage of virtual_memory ( 3rd field)\n",
    "print(\"RAM memory % used:\", psutil.virtual_memory()[2])\n",
    "# Getting usage of virtual_memory in GB ( 4th field)\n",
    "print(\"RAM Used (GB):\", psutil.virtual_memory()[3] / 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfIk2es3hJEd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmKRDJWgsFYa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from GAF.preprocessing.features_extractors.raw_extractor import RawExtractor\n",
    "from GAF.preprocessing.data_translator.gaf_translator import GafTranslator\n",
    "from GAF.preprocessing.preprocessor import Preprocessor\n",
    "\n",
    "SAMPLE_LENGTH_SECONDS = 2\n",
    "WINDOWS = 4\n",
    "BATCH_SIZE = 100\n",
    "EVALUATION_BATCH_SIZE = 4000\n",
    "RESAMPLE_MS = SAMPLE_LENGTH_SECONDS * 1000 // WINDOWS\n",
    "DATAPOINTS_PER_SECOND = 60\n",
    "DATA_PATH = \"/sise/yos-group/royhersh\"\n",
    "PATH = f\"{DATA_PATH}/data/processed/fixed_{DATAPOINTS_PER_SECOND}\"\n",
    "ANOMALIES_PATH = f\"{DATA_PATH}/anomalies/processed/fixed_{DATAPOINTS_PER_SECOND}\"\n",
    "AXIS_WINDOWS_AMOUNT = (\n",
    "    SAMPLE_LENGTH_SECONDS * DATAPOINTS_PER_SECOND\n",
    ")  # 4 windows * 0.500 s/window * 60 points-in-sample/s = 120 points-in-sample\n",
    "INPUT_SHAPE = (AXIS_WINDOWS_AMOUNT, AXIS_WINDOWS_AMOUNT)\n",
    "AXES = [\"x\", \"y\", \"z\", \"tot\"]\n",
    "\n",
    "extractor = RawExtractor(resample_amount=RESAMPLE_MS)\n",
    "preprocess = Preprocessor(\n",
    "    extractor,\n",
    "    translators=[GafTranslator(AXIS_WINDOWS_AMOUNT, WINDOWS)],\n",
    "    packed_windows=WINDOWS,\n",
    "    path=PATH,\n",
    ")\n",
    "anomalies_preprocess = Preprocessor(\n",
    "    extractor,\n",
    "    translators=[GafTranslator(AXIS_WINDOWS_AMOUNT, WINDOWS)],\n",
    "    packed_windows=WINDOWS,\n",
    "    path=ANOMALIES_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"\"\n",
    "FILE_TYPE = \"h5\"\n",
    "\n",
    "\n",
    "def define_output_path():\n",
    "    global OUTPUT_FILE\n",
    "    OUTPUT_FILE = (\n",
    "        \"models/gaf_autoencoders/{extractor}/{time}/\".format(\n",
    "            extractor=str(extractor),\n",
    "            time=datetime.datetime.now().strftime(\"%y.%m.%d/%H.%M\"),\n",
    "        )\n",
    "        + \"/{file_name}.{file_type}\"\n",
    "    )\n",
    "    print(OUTPUT_FILE)\n",
    "\n",
    "\n",
    "define_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmuCPVYKsKKx"
   },
   "outputs": [],
   "source": [
    "from GAF.preprocessing.gaf_dataset_loader import GAFDatasetLoader\n",
    "\n",
    "sensor = \"gyroscope\"\n",
    "translator = GafTranslator(AXIS_WINDOWS_AMOUNT, WINDOWS, method=\"summation\")\n",
    "# translator = GafTranslator(AXIS_WINDOWS_AMOUNT, WINDOWS, method='difference')\n",
    "train_dataset = GAFDatasetLoader(preprocess.get_file_name(sensor, [translator]), AXES, batch_size=BATCH_SIZE)\n",
    "anomaly_dataset = GAFDatasetLoader(\n",
    "    anomalies_preprocess.get_file_name(sensor, [translator]),\n",
    "    AXES,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_dataset_shuffle(indicies_path: str):\n",
    "    with open(indicies_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "        indicies = json.loads(content)\n",
    "    train_dataset._train_indices = np.array(indicies[\"benign_train\"])\n",
    "    train_dataset._test_indices = np.array(indicies[\"benign_test\"])\n",
    "\n",
    "    anomaly_dataset._train_indices = np.array(indicies[\"anomaly_train\"])\n",
    "    anomaly_dataset._test_indices = np.array(indicies[\"anomaly_test\"])\n",
    "\n",
    "\n",
    "# load_dataset_shuffle(\"train_tf/models/gaf_autoencoders/RawExtractor/23.09.11/07.23/train_test_indicies.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVcTBDo-CqFS"
   },
   "source": [
    "Plot a normal sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plot(title, data):\n",
    "    data = np.array(data).reshape(INPUT_SHAPE)\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.pcolor(\n",
    "        np.arange(0, AXIS_WINDOWS_AMOUNT, 1.0),\n",
    "        np.arange(0, AXIS_WINDOWS_AMOUNT, 1.0),\n",
    "        data,\n",
    "        cmap=\"rainbow\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def simple_subplot(title, data, subplot_data):\n",
    "    data = np.array(data).reshape(INPUT_SHAPE)\n",
    "    plt.subplot(*subplot_data)\n",
    "    plt.pcolor(\n",
    "        np.arange(0, AXIS_WINDOWS_AMOUNT, 1.0),\n",
    "        np.arange(0, AXIS_WINDOWS_AMOUNT, 1.0),\n",
    "        data,\n",
    "        cmap=\"rainbow\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def plot_groups(title_format, data):\n",
    "    FIG_SIZE = 4\n",
    "    fig_width = FIG_SIZE + 2\n",
    "    for i in range(min(3, len(data))):\n",
    "        plt.figure(figsize=(len(AXES) * fig_width, FIG_SIZE))\n",
    "        for axis in range(len(AXES)):\n",
    "            simple_subplot(\n",
    "                title_format.format(sensor=sensor, i=i, axis=AXES[axis]),\n",
    "                data[i][axis],\n",
    "                (1, 4, axis + 1),\n",
    "            )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def draw_data(title_format, dataset):\n",
    "    dataset._batch_size = 3\n",
    "    plot_groups(title_format, dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_data(\"Benign {sensor} sample {i} axis {axis}\", train_dataset.get_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpI9by2ZA0NN",
    "tags": []
   },
   "source": [
    "Plot an anomalous samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrpXREF2siBr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_data(\"Anomalous {sensor} sample {i} axis {axis}\", anomaly_dataset.get_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DS6QKZJslZz"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf6owZQDsp9y"
   },
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "    def __init__(self, encoding_size=300):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.encoding_size = encoding_size\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(\n",
    "                    input_shape=(*INPUT_SHAPE, 1),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    dtype=tf.dtypes.float32,\n",
    "                ),\n",
    "                layers.Conv2D(filters=1, kernel_size=(5, 5), strides=(2, 2), activation=\"relu\"),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(1800, activation=\"relu\"),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(encoding_size, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(1800, activation=\"relu\"),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(3600, activation=\"relu\"),  # manual adapt to INPUT_SHAPE\n",
    "                layers.Reshape((60, 60, 1)),\n",
    "                layers.Conv2DTranspose(\n",
    "                    filters=1,\n",
    "                    kernel_size=(3, 3),\n",
    "                    strides=(2, 2),\n",
    "                    activation=\"relu\",\n",
    "                    padding=\"same\",\n",
    "                ),\n",
    "                layers.Reshape(INPUT_SHAPE, dtype=tf.dtypes.float32),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # x = tf.expand_dims(x, axis=-1)\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def save_model(self, file_name: str, extra_detail: str = \"\"):\n",
    "        print(f\"Saving model {self} to {OUTPUT_FILE.format(file_name=f'{file_name}*', file_type=FILE_TYPE)}\")\n",
    "        self.encoder.save(\n",
    "            OUTPUT_FILE.format(file_name=f\"{file_name}_encoder{extra_detail}\", file_type=FILE_TYPE),\n",
    "            save_format=FILE_TYPE,\n",
    "        )\n",
    "        self.decoder.save(\n",
    "            OUTPUT_FILE.format(file_name=f\"{file_name}_decoder{extra_detail}\", file_type=FILE_TYPE),\n",
    "            save_format=FILE_TYPE,\n",
    "        )\n",
    "\n",
    "    def load_model(self, file_name: str, extra_detail: str = \"\"):\n",
    "        print(f\"Loading model from {OUTPUT_FILE.format(file_name=f'{file_name}*', file_type=FILE_TYPE)}\")\n",
    "        self.encoder = tf.keras.models.load_model(\n",
    "            OUTPUT_FILE.format(file_name=f\"{file_name}_encoder{extra_detail}\", file_type=FILE_TYPE)\n",
    "        )\n",
    "        self.decoder = tf.keras.models.load_model(\n",
    "            OUTPUT_FILE.format(file_name=f\"{file_name}_decoder{extra_detail}\", file_type=FILE_TYPE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectors:\n",
    "    def __init__(self, dataset, axes=AXES, encoding_size=300):\n",
    "        self.new_model = True\n",
    "        self.encoding_size = encoding_size\n",
    "        self._dataset = dataset\n",
    "        self._axes = axes\n",
    "        self._auto_encoders_amount = len(axes)\n",
    "        self._start_index = 0\n",
    "        self._auto_encoders = [AnomalyDetector(encoding_size) for _ in self.get_axis_range()]\n",
    "        for axis_autoencoder in self._auto_encoders:\n",
    "            axis_autoencoder.compile(optimizer=\"adam\", loss=tf.keras.losses.MeanSquaredError())\n",
    "            # axis_autoencoder.compile(optimizer='adam', loss=CustomAccuracy(), metrics=['mae', 'mse'])\n",
    "\n",
    "    def encoder(self, samples):\n",
    "        output = []\n",
    "        for index in self.get_axis_range():\n",
    "            samples_axis = np.asarray([data[index] for data in samples], dtype=np.float32)\n",
    "            output.append(self._auto_encoders[index].encoder(samples_axis).numpy())\n",
    "        return list(zip(*output))\n",
    "\n",
    "    def decoder(self, encoded_samples: np.ndarray) -> np.ndarray:\n",
    "        output = []\n",
    "        for index in self.get_axis_range():\n",
    "            encoded_samples_axis = np.asarray([data[index] for data in encoded_samples], dtype=np.float32)\n",
    "            output.append(self._auto_encoders[index].decoder(encoded_samples_axis).numpy())\n",
    "        return list(zip(*output))\n",
    "\n",
    "    def get_axis_range(self):\n",
    "        return range(self._start_index, self._auto_encoders_amount)\n",
    "\n",
    "    def call(self, samples):\n",
    "        output = []\n",
    "        for index in self.get_axis_range():\n",
    "            samples_axis = np.asarray([data[index] for data in samples], dtype=np.float32)\n",
    "            output.append(self._auto_encoders[index].call(samples_axis).numpy())\n",
    "        return list(zip(*output))\n",
    "\n",
    "    def train(self, epochs=50):\n",
    "        histories = [[] for _ in self.get_axis_range()]\n",
    "        train_data = self._dataset.get_train()\n",
    "        test_data = self._dataset.get_test()\n",
    "        length_train = len(train_data)\n",
    "        length_test = len(test_data)\n",
    "        # print(f\"Training data {length_train}\")\n",
    "        # print(f\"Test data {length_test}\")\n",
    "        for epoch in range(epochs):\n",
    "            # print(f\"Starting epoch {epoch}\")\n",
    "            for batch_index in range(length_train):\n",
    "                # print(f\"train_load {epoch}-{batch_index}/{length_train}\")\n",
    "                current_train = train_data[batch_index]\n",
    "                batch_loss = []\n",
    "                for axis in self.get_axis_range():\n",
    "                    train_cur = np.asarray([data[axis] for data in current_train], dtype=np.float32)\n",
    "                    assert (\n",
    "                        len(train_cur) == BATCH_SIZE\n",
    "                    ), f\"[x] No train data on {batch_index}-{axis} - {len(train_cur)}!={BATCH_SIZE} - {np.array(current_train).shape} {train_cur.shape}\"\n",
    "                    loss = self._auto_encoders[axis].train_on_batch(train_cur, train_cur)\n",
    "                    batch_loss.append(loss)\n",
    "                    histories[axis].append(loss)\n",
    "                # print(f\"Batch round loss {','.join(map(str, batch_loss))}\")\n",
    "            train_data.on_epoch_end()\n",
    "        return histories\n",
    "\n",
    "    def save_model(self, file_name: str):\n",
    "        for index, axis in enumerate(self._axes):\n",
    "            self._auto_encoders[index].save_model(file_name, axis)\n",
    "\n",
    "    def load_model(self, file_name: str):\n",
    "        self.new_model = False\n",
    "        for index, axis in enumerate(self._axes):\n",
    "            self._auto_encoders[index].load_model(file_name, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_file_name(autoencoder):\n",
    "    return f\"{sensor}_{extractor._ms_resample}x{preprocess._packed_windows}_{INPUT_SHAPE}_{autoencoder.encoding_size}\"\n",
    "\n",
    "\n",
    "def load_model(autoencoder, model_date: str):\n",
    "    global OUTPUT_FILE\n",
    "    OUTPUT_FILE = (\n",
    "        \"models/gaf_autoencoders/{extractor}/{time}\".format(extractor=str(extractor), time=model_date)\n",
    "        + \"/{file_name}.{file_type}\"\n",
    "    )\n",
    "    autoencoder.load_model(get_model_file_name(autoencoder))\n",
    "\n",
    "\n",
    "def save_model(autoencoder):\n",
    "    os.makedirs(pathlib.Path(OUTPUT_FILE).parent, exist_ok=True)\n",
    "    autoencoder.save_model(get_model_file_name(autoencoder))\n",
    "\n",
    "\n",
    "# load_model(autoencoder, \"11.06.23/23.02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuTy60STBEy4"
   },
   "source": [
    "Notice that the autoencoder is trained using only the normal samples, but is evaluated using the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEexphFwwTQS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_histories(histories):\n",
    "    for index in range(len(histories)):\n",
    "        plt.plot(histories[index], label=f\"Training Loss axis {AXES[index]}\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceI5lKv1BT-A",
    "tags": []
   },
   "source": [
    "You will soon classify an samples as anomalous if the reconstruction error is greater than one standard deviation from the normal training examples. First, let's plot a normal samples from the training set, the reconstruction after it's encoded and decoded by the autoencoder, and the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmsk4DuktxJ2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EVALUATION_BATCH_SIZE = BATCH_SIZE\n",
    "TRAIN_DATASET = train_dataset.get_train()\n",
    "TRAIN_DATASET._batch_size = EVALUATION_BATCH_SIZE\n",
    "TEST_DATASET = train_dataset.get_test()\n",
    "TEST_DATASET._batch_size = EVALUATION_BATCH_SIZE\n",
    "print(f\"Loading train_batch {EVALUATION_BATCH_SIZE}\")\n",
    "train_batch = TRAIN_DATASET[0]\n",
    "print(f\"Loading test_batch {EVALUATION_BATCH_SIZE}\")\n",
    "test_batch = TEST_DATASET[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_decoded_original(title, test_data, decoded_data):\n",
    "    simple_plot(f\"Input {title}\", test_data)\n",
    "    simple_plot(f\"Reconstructed {title}\", decoded_data)\n",
    "\n",
    "\n",
    "def plot_original_vs_reconstructed(title_format, reconstructed_title_format, data, reconstructed_data):\n",
    "    FIG_SIZE = 9\n",
    "    fig_width = FIG_SIZE - 3\n",
    "    for i in range(min(3, len(data))):\n",
    "        plt.figure(figsize=(len(AXES) * fig_width, FIG_SIZE))\n",
    "        for axis in range(len(AXES)):\n",
    "            simple_subplot(\n",
    "                title_format.format(sensor=sensor, i=i, axis=AXES[axis]),\n",
    "                data[i][axis],\n",
    "                (2, len(AXES), axis + 1),\n",
    "            )\n",
    "        for axis in range(len(AXES)):\n",
    "            simple_subplot(\n",
    "                reconstructed_title_format.format(sensor=sensor, i=i, axis=AXES[axis]),\n",
    "                reconstructed_data[i][axis],\n",
    "                (2, len(AXES), axis + 1 + len(AXES)),\n",
    "            )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_plot(autoencoder, data_batch, title_format, reconstructed_title_format):\n",
    "    print(f\"Encode {len(data_batch)}\")\n",
    "    encoded_data = autoencoder.encoder(data_batch)\n",
    "    print(f\"Decode {len(encoded_data)}\")\n",
    "    decoded_data = autoencoder.decoder(encoded_data)\n",
    "    print(\n",
    "        np.array(test_batch).shape,\n",
    "        np.array(encoded_data).shape,\n",
    "        np.array(decoded_data).shape,\n",
    "    )\n",
    "    plot_original_vs_reconstructed(title_format, reconstructed_title_format, test_batch, decoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocA_q9ufB_aF"
   },
   "source": [
    "Create a similar plot, this time for an anomalous test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNFTuPhLwTBn"
   },
   "outputs": [],
   "source": [
    "ANOMALY_TRAIN_DATASET = anomaly_dataset.get_train()\n",
    "ANOMALY_TRAIN_DATASET._batch_size = EVALUATION_BATCH_SIZE\n",
    "ANOMALY_TEST_DATASET = anomaly_dataset.get_test()\n",
    "ANOMALY_TEST_DATASET._batch_size = EVALUATION_BATCH_SIZE\n",
    "print(f\"Loading anomaly_train_batch {EVALUATION_BATCH_SIZE}\")\n",
    "anomaly_train_batch = ANOMALY_TRAIN_DATASET[0]\n",
    "print(f\"Loading anomaly_test_batch {EVALUATION_BATCH_SIZE}\")\n",
    "anomaly_test_batch = ANOMALY_TEST_DATASET[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocimg3MBswdS"
   },
   "source": [
    "### Detect anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xnh8wmkDsypN"
   },
   "source": [
    "Detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. In this tutorial, you will calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeuT8uTA5Y_w"
   },
   "source": [
    "Plot the reconstruction error on normal samples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_ARRANGE_SIZE = AXIS_WINDOWS_AMOUNT * AXIS_WINDOWS_AMOUNT\n",
    "\n",
    "\n",
    "def calculate_loss(model, data):\n",
    "    print(data.shape)\n",
    "    reconstructions = model.call(data)\n",
    "    print(np.array(reconstructions).shape)\n",
    "    # custom_acc = CustomAccuracy()\n",
    "    custom_acc = tf.keras.losses.MeanSquaredError()\n",
    "    amount, axes, _data1, _data2 = data.shape\n",
    "    loss = []\n",
    "    for i in range(amount):\n",
    "        tmp_loss = 0\n",
    "        for axis in model.get_axis_range():\n",
    "            tmp_loss += custom_acc.call(\n",
    "                np.array(reconstructions[i][axis - model._start_index]).reshape(LOSS_ARRANGE_SIZE),\n",
    "                np.array(data[i][axis]).reshape(LOSS_ARRANGE_SIZE),\n",
    "            )\n",
    "        loss.append(tmp_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_span(autoencoder, data_batch, title=\"\"):\n",
    "    train_loss = calculate_loss(autoencoder, data_batch)\n",
    "    loss_mean = np.mean(train_loss)\n",
    "    loss_std = np.std(train_loss)\n",
    "    plt.hist(train_loss, bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.ylabel(\"No of examples\")\n",
    "    plt.show()\n",
    "    return loss_mean, loss_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a threshold value that is one standard deviations above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -1\n",
    "anomaly_threshold = -1\n",
    "anomaly_benign_test_span_distance = -1\n",
    "benign_train_span = \"\"\n",
    "benign_test_span = \"\"\n",
    "anomaly_train_span = \"\"\n",
    "\n",
    "\n",
    "def calculate_thresholds(autoencoder):\n",
    "    global threshold, anomaly_threshold, benign_train_span, benign_test_span, anomaly_train_span\n",
    "    tmp_mean, tmp_std = calculate_loss_span(autoencoder, train_batch, \"Benign train loss\")\n",
    "    benign_train_span = f\"[{tmp_mean - tmp_std}-{tmp_mean + tmp_std}]\"\n",
    "    print(\"Train loss span: \", benign_train_span)\n",
    "    threshold = tmp_mean + tmp_std\n",
    "    print(\"Benign train threshold: \", threshold)\n",
    "\n",
    "    tmp_mean, tmp_std = calculate_loss_span(autoencoder, test_batch, \"Benign test loss\")\n",
    "    benign_test_upper_loss_span = tmp_mean + tmp_std\n",
    "    benign_test_span = f\"[{tmp_mean - tmp_std}-{benign_test_upper_loss_span}]\"\n",
    "    print(\"Benign test loss span: \", benign_test_span)\n",
    "\n",
    "    tmp_mean, tmp_std = calculate_loss_span(autoencoder, anomaly_train_batch, \"Anomaly train loss\")\n",
    "    anomaly_train_span = f\"[{tmp_mean - tmp_std}-{tmp_mean + tmp_std}]\"\n",
    "    print(\n",
    "        \"benign train-test, anomalous-train span: \",\n",
    "        benign_train_span,\n",
    "        benign_test_span,\n",
    "        anomaly_train_span,\n",
    "    )\n",
    "    anomaly_lower_span = tmp_mean - tmp_std\n",
    "    anomaly_threshold = anomaly_lower_span\n",
    "    anomaly_benign_test_span_distance = anomaly_lower_span - benign_test_upper_loss_span\n",
    "    print(\"Anomaly train threshold: \", anomaly_lower_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEGlA1Be50Nj"
   },
   "source": [
    "Note: There are other strategies you could use to select a threshold value above which test examples should be classified as anomalous, the correct approach will depend on your dataset. You can learn more with the links at the end of this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the reconstruction error for the anomalous examples in the test set, you'll notice most have greater reconstruction error than the threshold. By varing the threshold, you can adjust the [precision](https://developers.google.com/machine-learning/glossary#precision) and [recall](https://developers.google.com/machine-learning/glossary#recall) of your classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFVk_XGE6AX2"
   },
   "source": [
    "Classify an samples as an anomaly if the reconstruction error is greater than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkgJZfhh6CHr"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "def predict(model, data, threshold):\n",
    "    loss = calculate_loss(model, data)\n",
    "    return tf.math.less(loss, threshold)\n",
    "\n",
    "\n",
    "# cannot memoize without threshold with the following memoize function\n",
    "def get_stats(predictions, labels):\n",
    "    return f\"Accuracy = {accuracy_score(labels, predictions)}\"\n",
    "\n",
    "\n",
    "def get_stats_all(predictions, labels):\n",
    "    return f\"\"\"{get_stats(predictions, labels)}\n",
    "Precision = {precision_score(labels, predictions)}\n",
    "Recall = {recall_score(labels, predictions)}\n",
    "F1 = {f1_score(labels, predictions)}\n",
    "confusion_matrix -\\n{confusion_matrix(labels, predictions)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOcfXfXq6FBd"
   },
   "outputs": [],
   "source": [
    "from itertools import repeat, chain\n",
    "\n",
    "\n",
    "def evaluate(model, threshold):\n",
    "    output = f\"\\n[*] Calculating result with threshold {threshold}\"\n",
    "    predications_benign = predict(model, test_batch, threshold)\n",
    "    labels_benign = list(repeat(True, len(predications_benign)))\n",
    "    stats_benign = get_stats(predications_benign, labels_benign)\n",
    "    output += \"\\n~ Benign test performance ~\\n\" + stats_benign\n",
    "\n",
    "    predications_anomaly = predict(model, anomaly_train_batch, threshold)\n",
    "    labels_anomaly = list(repeat(False, len(predications_anomaly)))\n",
    "    stats_anomaly = get_stats(predications_anomaly, labels_anomaly)\n",
    "    output += \"\\n~ Anomaly train performance ~\\n\" + stats_anomaly\n",
    "\n",
    "    predications_anomaly_test = predict(model, anomaly_test_batch, threshold)\n",
    "    labels_anomaly_test = list(repeat(False, len(predications_anomaly_test)))\n",
    "    stats_anomaly = get_stats(predications_anomaly_test, labels_anomaly_test)\n",
    "    output += \"\\n~ Anomaly test performance ~\\n\" + stats_anomaly\n",
    "\n",
    "    predications_total = list(chain(predications_benign, predications_anomaly, predications_anomaly_test))\n",
    "    labels_total = list(chain(labels_benign, labels_anomaly, labels_anomaly_test))\n",
    "    stats_total = get_stats_all(predications_total, labels_total)\n",
    "    output += \"\\nTotal performance: \\n\" + stats_total\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(autoencoder, file_name: str):\n",
    "    output_filename = OUTPUT_FILE.format(file_name=file_name, file_type=\"log\")\n",
    "    with open(output_filename, \"w+\") as f:\n",
    "        date = datetime.datetime.now().strftime(\"%d/%m/%y %H:%M\")\n",
    "        f.write(\"\\n\\n~~~ Model Results ~~~\\n\")\n",
    "        f.write(f\"Finish date: {date}\\n\")\n",
    "        f.write(f\"Output directory: {output_filename}\\n\")\n",
    "        f.write(f\"AXES: {AXES}\\n\")\n",
    "        f.write(f\"Sample length seconds: {SAMPLE_LENGTH_SECONDS}\\n\")\n",
    "        f.write(f\"Enforced datepoints per second: {DATAPOINTS_PER_SECOND}\\n\")\n",
    "        f.write(f\"Resample: {RESAMPLE_MS}\\n\")\n",
    "        f.write(f\"INPUT_SHAPE: {INPUT_SHAPE}\\n\")\n",
    "\n",
    "        f.write(\"\\n\\n!! Model Info !!\\n\")\n",
    "        sub_autoencoder = autoencoder._auto_encoders[0]\n",
    "        f.write(f\"ENCODING_SIZE: {sub_autoencoder.encoding_size}\\n\")\n",
    "        f.write(f\"Encoder:\\n\")\n",
    "        sub_autoencoder.encoder.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "        f.write(f\"Decoder:\\n\")\n",
    "        sub_autoencoder.decoder.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "        f.write(\"\\n\\n!! Dataset Info !!\\n\")\n",
    "        f.write(f\"Sensor: {sensor}\\n\")\n",
    "        f.write(f\"Train: <data: {len(train_dataset.get_train())}>, Test:<data: {len(train_dataset.get_test())}>\\n\")\n",
    "        f.write(\n",
    "            f\"Anomalies: <data: {len(anomaly_dataset.get_train())}>, Test:<data: {len(anomaly_dataset.get_test())}>\\n\"\n",
    "        )\n",
    "        f.write(f\"Benign train span: {benign_train_span}\\n\")\n",
    "        f.write(f\"Benign test span: {benign_test_span}\\n\")\n",
    "        f.write(f\"Anomaly train span: {anomaly_train_span}\\n\")\n",
    "        f.write(f\"Anomaly-train:Benign-test span distance: {anomaly_benign_test_span_distance}\\n\")\n",
    "\n",
    "        f.write(\"\\n\\n!! First evaluation !!\\n\")\n",
    "        f.write(evaluate(autoencoder, threshold))\n",
    "        f.write(\"\\n\\n!! Second evaluation !!\\n\")\n",
    "        f.write(evaluate(autoencoder, anomaly_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axes(autoencoder, axes, start_index, end_index):\n",
    "    autoencoder._axes = axes\n",
    "    autoencoder._auto_encoders_amount = end_index\n",
    "    autoencoder._start_index = start_index\n",
    "    set_axes_dataset(axes)\n",
    "\n",
    "\n",
    "def set_axes_dataset(axes):\n",
    "    global train_dataset, anomaly_dataset\n",
    "    train_dataset._axes = axes\n",
    "    anomaly_dataset._axes = axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_save_results(axes, encoding_size):\n",
    "    autoencoder = AnomalyDetectors(train_dataset, axes, encoding_size)\n",
    "    histories = autoencoder.train(epochs=30)\n",
    "    plot_histories(histories)\n",
    "    encode_decode_plot(\n",
    "        autoencoder,\n",
    "        test_batch[0:3],\n",
    "        \"Input {sensor} sample {i} axis {axis}\",\n",
    "        \"Reconstructed {sensor} sample {i} axis {axis}\",\n",
    "    )\n",
    "    encode_decode_plot(\n",
    "        autoencoder,\n",
    "        anomaly_train_batch[0:3],\n",
    "        \"Input {sensor} anomalous sample {i} axis {axis}\",\n",
    "        \"Reconstructed {sensor} anomalous sample {i} axis {axis}\",\n",
    "    )\n",
    "    calculate_thresholds(autoencoder)\n",
    "    # print(evaluate(autoencoder, anomaly_threshold))\n",
    "    if autoencoder.new_model:\n",
    "        save_model(autoencoder)\n",
    "        log_results(autoencoder, f\"{'_'.join(AXES)}_{get_model_file_name(autoencoder)}\")\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_model_and_evaluate_axes(encoding_size):\n",
    "    global AXES\n",
    "    AXES = [\"x\", \"y\", \"z\", \"tot\"]\n",
    "    set_axes_dataset(AXES)\n",
    "    autoencoder = train_model_and_save_results(AXES, encoding_size)\n",
    "\n",
    "    AXES = [\"x\", \"y\", \"z\"]\n",
    "    set_axes(autoencoder, AXES, 0, len(AXES))\n",
    "    calculate_thresholds(autoencoder)\n",
    "    log_results(autoencoder, f\"{'_'.join(AXES)}_{get_model_file_name(autoencoder)}\")\n",
    "\n",
    "    AXES = [\"tot\"]\n",
    "    set_axes(autoencoder, AXES, 3, 4)\n",
    "    calculate_thresholds(autoencoder)\n",
    "    log_results(autoencoder, f\"{'_'.join(AXES)}_{get_model_file_name(autoencoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_encoding_size():\n",
    "    for encoding_size in ENCODING_SIZES:\n",
    "        print(datetime.datetime.now().strftime(f\"{encoding_size}-started at: %d.%m.%y-%H:%M\"))\n",
    "        define_output_path()\n",
    "        produce_model_and_evaluate_axes(encoding_size)\n",
    "        print(datetime.datetime.now().strftime(f\"{encoding_size}-ended at: %d.%m.%y-%H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def save_train_test_idicies(file_name: str = \"train_test_indicies\"):\n",
    "    out = {\n",
    "        \"benign_train\": train_dataset._train_indices.tolist(),\n",
    "        \"benign_test\": train_dataset._test_indices.tolist(),\n",
    "        \"anomaly_train\": anomaly_dataset._train_indices.tolist(),\n",
    "        \"anomaly_test\": anomaly_dataset._test_indices.tolist(),\n",
    "    }\n",
    "    os.makedirs(pathlib.Path(OUTPUT_FILE).parent, exist_ok=True)\n",
    "    with open(OUTPUT_FILE.format(file_name=file_name, file_type=\"json\"), \"w+\") as f:\n",
    "        f.write(json.dumps(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_test_idicies(f\"train_test_indicies_{DATAPOINTS_PER_SECOND}\")\n",
    "ENCODING_SIZES = [250, 300, 400, 500, 600, 900, 1200]\n",
    "run_for_encoding_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for autosave\n",
    "import time\n",
    "\n",
    "time.sleep(180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cancel_str = \"scancel \" + os.environ[\"SLURM_JOBID\"]\n",
    "os.system(job_cancel_str)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
