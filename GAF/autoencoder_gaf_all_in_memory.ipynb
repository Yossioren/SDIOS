{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfIk2es3hJEd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmKRDJWgsFYa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from GAF.preprocessing.features_extractors.raw_extractor import RawExtractor\n",
    "from GAF.preprocessing.data_translator.gaf_translator import GafTranslator\n",
    "from GAF.preprocessing.preprocessor import Preprocessor\n",
    "\n",
    "TIME_PER_SAMPLE = 2\n",
    "WINDOWS = 4\n",
    "BATCH_SIZE = 800\n",
    "RESAMPLE_MS = TIME_PER_SAMPLE * 1000 // WINDOWS\n",
    "SAMPLES_PER_MINUTE = 60\n",
    "PATH = f\"/sise/yos-group/royhersh/data/processed/fixed_{SAMPLES_PER_MINUTE}\"\n",
    "ANOMALIES_PATH = f\"/sise/yos-group/royhersh/anomalies/processed/fixed_{SAMPLES_PER_MINUTE}\"\n",
    "AXIS_WINDOWS_AMOUNT = (\n",
    "    TIME_PER_SAMPLE * SAMPLES_PER_MINUTE\n",
    ")  # 4 windows * 0.500 s/window * 60 points-in-sample/s = 120 points-in-sample\n",
    "AXES = [\"x\", \"y\", \"z\", \"tot\"]\n",
    "\n",
    "extractor = RawExtractor(resample_amount=RESAMPLE_MS)\n",
    "preprocess = Preprocessor(\n",
    "    extractor,\n",
    "    translators=[GafTranslator(AXIS_WINDOWS_AMOUNT, WINDOWS)],\n",
    "    packed_windows=WINDOWS,\n",
    "    path=PATH,\n",
    ")\n",
    "anomalies_preprocess = Preprocessor(\n",
    "    extractor,\n",
    "    translators=[GafTranslator(AXIS_WINDOWS_AMOUNT, WINDOWS)],\n",
    "    packed_windows=WINDOWS,\n",
    "    path=ANOMALIES_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "OUTPUT_FILE = (\n",
    "    \"gaf_autoencoders/{extractor}/{time}/\".format(\n",
    "        extractor=str(extractor), time=datetime.datetime.now().strftime(\"%d.%m.%y\")\n",
    "    )\n",
    "    + \"/{file_name}.{file_type}\"\n",
    ")\n",
    "os.makedirs(pathlib.Path(OUTPUT_FILE).parent, exist_ok=True)\n",
    "FILE_TYPE = \"h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmuCPVYKsKKx"
   },
   "outputs": [],
   "source": [
    "sensor = \"gyroscope\"\n",
    "\n",
    "\n",
    "def load_dataset(preprocess, sensor):\n",
    "    data, labels, info = preprocess.load_dataset_sensor(sensor)\n",
    "\n",
    "    print(\"[*] Spliting test/train\")\n",
    "    return preprocess.create_dataset(data, labels, info, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_train_data, _, anomaly_test_data, _, _, _ = load_dataset(anomalies_preprocess, sensor)\n",
    "train_data, train_labels, test_data, test_labels, _, _ = load_dataset(preprocess, sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVcTBDo-CqFS"
   },
   "source": [
    "Plot a normal sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTlMIrpmseYe"
   },
   "outputs": [],
   "source": [
    "def simple_plot(title, data):\n",
    "    data = np.array(data).reshape(AXIS_WINDOWS_AMOUNT, AXIS_WINDOWS_AMOUNT)\n",
    "    plt.clf()\n",
    "    plt.title(title)\n",
    "    fig = plt.figure(1, dpi=100)\n",
    "\n",
    "    width_ratios = (0.4, 7, 0.4)\n",
    "    height_ratios = (0.4, 7)\n",
    "    # width = 10\n",
    "    # height = width * sum(height_ratios) / sum(width_ratios)\n",
    "    # plt.gcf().set_size_inches(width, height)\n",
    "    plt.title(title)\n",
    "    gs = fig.add_gridspec(\n",
    "        2,\n",
    "        3,\n",
    "        width_ratios=width_ratios,\n",
    "        height_ratios=height_ratios,\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        bottom=0.1,\n",
    "        top=0.9,\n",
    "        wspace=0.1,\n",
    "        hspace=0.1,\n",
    "    )\n",
    "\n",
    "    # Plot the Gramian angular fields on the bottom right\n",
    "    ax_gasf = fig.add_subplot(gs[1, 1])\n",
    "    ax_gasf.imshow(data, cmap=\"rainbow\", origin=\"lower\", extent=[0, 4 * np.pi, 0, 4 * np.pi])\n",
    "    ax_gasf.set_xticks([])\n",
    "    ax_gasf.set_yticks([])\n",
    "\n",
    "    # Add colorbar\n",
    "    im = ax_gasf.imshow(data, cmap=\"rainbow\", origin=\"lower\", extent=[0, 4 * np.pi, 0, 4 * np.pi])\n",
    "    ax_cbar = fig.add_subplot(gs[1, 2])\n",
    "    fig.colorbar(im, cax=ax_cbar)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for axis in range(len(AXES)):\n",
    "        simple_plot(f\"A gyroscope {axis} sample {i}\", train_data[i][axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpI9by2ZA0NN"
   },
   "source": [
    "Plot an anomalous ECG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrpXREF2siBr"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for axis in range(len(AXES)):\n",
    "        simple_plot(f\"An anomalous gyroscope {axis} sample {i}\", anomaly_train[i][axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DS6QKZJslZz"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf6owZQDsp9y"
   },
   "outputs": [],
   "source": [
    "class CustomAccuracy(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=0.0001):\n",
    "        super().__init__()\n",
    "        self._threshold = threshold\n",
    "        self._inv_threshold = 1 / threshold\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        diff = tf.reduce_mean(tf.math.abs(y_pred - y_true))\n",
    "        if diff > self._threshold:\n",
    "            return tf.square(self._inv_threshold * diff)\n",
    "        return diff\n",
    "\n",
    "\n",
    "class AnomalyDetector(Model):\n",
    "    def __init__(self):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(300, activation=\"relu\"),\n",
    "                layers.Dropout(0.1),\n",
    "                layers.Dense(200, activation=\"relu\"),\n",
    "                layers.Dropout(0.1),\n",
    "                layers.Dense(120, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(200, activation=\"relu\"),\n",
    "                layers.Dropout(0.1),\n",
    "                layers.Dense(300, activation=\"relu\"),\n",
    "                layers.Dropout(0.1),\n",
    "                layers.Dense(AXIS_WINDOWS_AMOUNT, activation=\"sigmoid\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def save_model(self, file_name: str, extra_detail: str = \"\"):\n",
    "        self.encoder.save(\n",
    "            OUTPUT_FILE.format(file_name=f\"{file_name}_encoder{extra_detail}\", file_type=FILE_TYPE),\n",
    "            save_format=FILE_TYPE,\n",
    "        )\n",
    "        self.decoder.save(\n",
    "            OUTPUT_FILE.format(file_name=f\"{file_name}_decoder{extra_detail}\", file_type=FILE_TYPE),\n",
    "            save_format=FILE_TYPE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectors:\n",
    "    def __init__(self, train_data, test_data, axes=AXES):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._axes = axes\n",
    "        self._autoencoders = [AnomalyDetector() for _ in range(len(axes))]\n",
    "        for autoencoder in self._autoencoders:\n",
    "            autoencoder.compile(optimizer=\"adam\", loss=tf.keras.losses.MeanSquaredError())\n",
    "            # autoencoder.compile(optimizer='adam', loss=CustomAccuracy(), metrics=['mae', 'mse'])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        output = []\n",
    "        for index in range(len(self._autoencoders)):\n",
    "            tmp = [array[index] for array in x]\n",
    "            output.append(self._autoencoders[index].encoder(tmp).numpy())\n",
    "        return output\n",
    "\n",
    "    def decoder(self, x):\n",
    "        output = []\n",
    "        for index in range(len(self._autoencoders)):\n",
    "            tmp = [array[index] for array in x]\n",
    "            output.append(self._autoencoders[index].decoder(tmp).numpy())\n",
    "        return output\n",
    "\n",
    "    def call(self, x):\n",
    "        output = []\n",
    "        for index in range(len(self._autoencoders)):\n",
    "            tmp = [array[index] for array in x]\n",
    "            output.append(self._autoencoders[index].call(tmp))\n",
    "        return output\n",
    "\n",
    "    def train(self, epochs):\n",
    "        histories = [[] for _ in range(len(self._autoencoders))]\n",
    "        length_train = len(self._train_data)\n",
    "        length_test = len(test_data)\n",
    "        print(f\"Training data {length_train}\")\n",
    "        print(f\"Test data {length_test}\")\n",
    "        # test_data._batch_size = BATCH_SIZE * 5\n",
    "        validation_data = [[] for _ in range(len(self._autoencoders))]\n",
    "        for axis in range(len(self._autoencoders)):\n",
    "            test_cur = np.asarray([data[axis] for self._ in self._test_data], dtype=np.float32)\n",
    "            validation_data[axis] = (test_cur, test_cur)\n",
    "            train_cur = np.asarray([data[axis] for data in self._train_data], dtype=np.float32)\n",
    "            print(f\"train_load {i}/{length_train} {axis}\")\n",
    "            history = self._autoencoders[axis].fit(\n",
    "                train_cur,\n",
    "                train_cur,\n",
    "                epochs=epochs,\n",
    "                shuffle=False,\n",
    "                validation_data=validation_data[axis],\n",
    "            )\n",
    "            histories[axis].append(history)\n",
    "        return histories\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        for index in range(len(self._autoencoders)):\n",
    "            self._auto_encoders[index].save_model(sensor, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_date):\n",
    "    global OUTPUT_FILE\n",
    "    OUTPUT_FILE = (\n",
    "        \"autoencoders/{preprocessor}/{time}/\".format(preprocessor=str(preprocess), time=model_date)\n",
    "        + \"/{file_name}.{file_type}\"\n",
    "    )\n",
    "    model_file_name = f\"{sensor}_{extractor._ms_resample}x{preprocess._packed_windows}\"\n",
    "    autoencoder.load_model(model_file_name)\n",
    "\n",
    "\n",
    "new_model = False\n",
    "# load_model(\"18.03.23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwRpBBbg463S"
   },
   "outputs": [],
   "source": [
    "new_model = True\n",
    "autoencoder = AnomalyDetectors(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuTy60STBEy4"
   },
   "source": [
    "Notice that the autoencoder is trained using only the normal ECGs, but is evaluated using the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6NFSs-jsty2"
   },
   "outputs": [],
   "source": [
    "histories = autoencoder.train(epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEexphFwwTQS"
   },
   "outputs": [],
   "source": [
    "for history_list in histories:\n",
    "    total_loss = []\n",
    "    total_val_loss = []\n",
    "    for history in history_list:\n",
    "        total_loss.extend(history.history[\"loss\"])\n",
    "        total_val_loss.extend(history.history[\"val_loss\"])\n",
    "    plt.plot(total_loss, label=\"Training Loss\")\n",
    "    plt.plot(total_val_loss, label=\"Validation Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceI5lKv1BT-A"
   },
   "source": [
    "You will soon classify an ECG as anomalous if the reconstruction error is greater than one standard deviation from the normal training examples. First, let's plot a normal ECG from the training set, the reconstruction after it's encoded and decoded by the autoencoder, and the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmsk4DuktxJ2"
   },
   "outputs": [],
   "source": [
    "train_batch = train_dataset.get_train()[0]\n",
    "test_batch = train_dataset.get_test()[0]\n",
    "encoded_data = autoencoder.encoder(test_batch)\n",
    "decoded_data = autoencoder.decoder(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decoded_original(title, test_data, decoded_data):\n",
    "    simple_plot(f\"Input {title}\", test_data)\n",
    "    simple_plot(f\"Reconstructed {title}\", decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for axis in range(len(AXES)):\n",
    "        plot_decoded_original(f\"{sensor} {axis} sample {i}\", test_batch[i][axis], decoded_data[i][axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocA_q9ufB_aF"
   },
   "source": [
    "Create a similar plot, this time for an anomalous test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNFTuPhLwTBn"
   },
   "outputs": [],
   "source": [
    "anomaly_train_batch = anomaly_dataset.get_train()[0]\n",
    "anomaly_test_batch = anomaly_dataset.get_test()[0]\n",
    "anomaly_encoded_data = autoencoder.encoder(anomaly_train_batch)\n",
    "anomaly_decoded_data = autoencoder.decoder(anomaly_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for axis in range(len(AXES)):\n",
    "        plot_decoded_original(\n",
    "            f\"{sensor} {axis} anomalous sample {i}\",\n",
    "            anomaly_test_batch[i][axis],\n",
    "            anomaly_decoded_data[i][axis],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocimg3MBswdS"
   },
   "source": [
    "### Detect anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xnh8wmkDsypN"
   },
   "source": [
    "Detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. In this tutorial, you will calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeuT8uTA5Y_w"
   },
   "source": [
    "Plot the reconstruction error on normal ECGs from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data):\n",
    "    print(data.shape)\n",
    "    reconstructions = model.call(data)\n",
    "    print(np.array(reconstructions).shape)\n",
    "    # custom_acc = CustomAccuracy()\n",
    "    custom_acc = tf.keras.losses.MeanSquaredError()\n",
    "    amount, axes, _data1, _data2 = data.shape\n",
    "    loss = []\n",
    "    for i in range(amount):\n",
    "        tmp_loss = 0\n",
    "        for axis in range(axes):\n",
    "            tmp_loss += custom_acc.call(reconstructions[i][axis], data[i][axis])\n",
    "        loss.append(tmp_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7FltOnHu4-l"
   },
   "outputs": [],
   "source": [
    "train_loss = calculate_loss(autoencoder, train_batch)\n",
    "\n",
    "plt.hist(train_loss, bins=50)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss[0])\n",
    "for axis in range(len(AXES)):\n",
    "    print(\n",
    "        f\"custom loss function result example: {i}\",\n",
    "        custom_acc.call(reconstructions[axis][0], anomaly_train_batch[0][axis]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh-3ChEF5hog"
   },
   "source": [
    "Choose a threshold value that is one standard deviations above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82hkl0Chs3P_"
   },
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Benign threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = calculate_loss(autoencoder, test_batch)\n",
    "\n",
    "print(\n",
    "    \"Span: \",\n",
    "    np.mean(test_loss) - np.std(test_loss),\n",
    "    np.mean(test_loss) + np.std(test_loss),\n",
    ")\n",
    "\n",
    "plt.hist(test_loss, bins=50)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEGlA1Be50Nj"
   },
   "source": [
    "Note: There are other strategies you could use to select a threshold value above which test examples should be classified as anomalous, the correct approach will depend on your dataset. You can learn more with the links at the end of this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpLSDAeb51D_"
   },
   "source": [
    "If you examine the reconstruction error for the anomalous examples in the test set, you'll notice most have greater reconstruction error than the threshold. By varing the threshold, you can adjust the [precision](https://developers.google.com/machine-learning/glossary#precision) and [recall](https://developers.google.com/machine-learning/glossary#recall) of your classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKVwjQK955Wy"
   },
   "outputs": [],
   "source": [
    "anomaly_test_loss = calculate_loss(autoencoder, anomaly_train_batch)\n",
    "\n",
    "anomaly_lower_span = np.mean(anomaly_test_loss) - np.std(anomaly_test_loss)\n",
    "anomaly_upper_span = np.mean(anomaly_test_loss) + np.std(anomaly_test_loss)\n",
    "print(\"Span: \", anomaly_lower_span, anomaly_upper_span)\n",
    "\n",
    "anomaly_threshold = anomaly_lower_span\n",
    "\n",
    "plt.hist(anomaly_test_loss, bins=50)\n",
    "plt.xlabel(\"Anomaly test loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFVk_XGE6AX2"
   },
   "source": [
    "Classify an ECG as an anomaly if the reconstruction error is greater than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def memoize(function):\n",
    "    memo = {}\n",
    "\n",
    "    def wrapper(*args):\n",
    "        hash_str = \"\"\n",
    "        for arg in args:\n",
    "            arg_type = type(arg)\n",
    "            if arg_type == bytes or arg_type == str:\n",
    "                hash_str += f\"{str(arg[0])}{str(arg[-1])}{len(arg)}_\"\n",
    "            elif arg_type == list or arg_type == tuple:\n",
    "                # print(\"[x] Cannot hash well list and tuples\")\n",
    "                tmp_arg = arg\n",
    "                try:\n",
    "                    while True:\n",
    "                        hash_str += f\"{type(tmp_arg[0])}{len(tmp_arg)}_\"\n",
    "                        tmp_arg = tmp_arg[0]\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif arg_type == np.ndarray or arg_type == np.array:\n",
    "                # print(\"[x] Cannot hash well arrays\")\n",
    "                hash_str += f\"{str(arg.max())}{str(arg.min())}{arg.shape}_\"\n",
    "            elif arg_type == object:\n",
    "                hash_str += f\"{arg_type}_\"\n",
    "            else:\n",
    "                hash_str += f\"{str(arg)}_\"\n",
    "        # print(hash_str)\n",
    "        if hash_str in memo:\n",
    "            return memo[hash_str]\n",
    "        else:\n",
    "            rv = function(*args)\n",
    "            memo[hash_str] = rv\n",
    "            return rv\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkgJZfhh6CHr"
   },
   "outputs": [],
   "source": [
    "@memoize\n",
    "def predict(model, data, threshold):\n",
    "    loss = calculate_loss(model, data)\n",
    "    return tf.math.less(loss, threshold)\n",
    "\n",
    "\n",
    "# cannot memoize without threshold with the following memoize function\n",
    "def get_stats(predictions, labels):\n",
    "    return f\"Accuracy = {accuracy_score(labels, predictions)}\"\n",
    "\n",
    "\n",
    "def get_stats_all(predictions, labels):\n",
    "    return f\"\"\"{get_stats(predictions, labels)}\n",
    "Precision = {precision_score(labels, predictions)}\n",
    "Recall = {recall_score(labels, predictions)}\n",
    "F1 = {f1_score(labels, predictions)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOcfXfXq6FBd"
   },
   "outputs": [],
   "source": [
    "from itertools import repeat, chain\n",
    "\n",
    "\n",
    "def evaluate(model, threshold):\n",
    "    output = f\"\\n[*] Calculating result with threshold {threshold}\"\n",
    "    predications_benign = predict(model, test_batch, threshold)\n",
    "    labels_benign = list(repeat(True, len(predications_benign)))\n",
    "    stats_benign = get_stats(predications_benign, labels_benign)\n",
    "    output += \"\\nBenign test performance: \\n\" + stats_benign\n",
    "\n",
    "    predications_anomaly = predict(model, anomaly_train_batch, threshold)\n",
    "    labels_anomaly = list(repeat(False, len(predications_anomaly)))\n",
    "    stats_anomaly = get_stats(predications_anomaly, labels_anomaly)\n",
    "    output += \"\\nAnomaly train performance: \\n\" + stats_anomaly\n",
    "\n",
    "    predications_anomaly_test = predict(model, anomaly_test_batch, threshold)\n",
    "    labels_anomaly_test = list(repeat(False, len(predications_anomaly_test)))\n",
    "    stats_anomaly = get_stats(predications_anomaly_test, labels_anomaly_test)\n",
    "    output += \"\\nAnomaly test performance: \\n\" + stats_anomaly\n",
    "\n",
    "    predications_total = list(chain(predications_benign, predications_anomaly, predications_anomaly_test))\n",
    "    labels_total = list(chain(labels_benign, labels_anomaly, labels_anomaly_test))\n",
    "    stats_total = get_stats_all(predications_total, labels_total)\n",
    "    output += \"\\nTotal performance: \\n\" + stats_total\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(evaluate(autoencoder, threshold))\n",
    "print(evaluate(autoencoder, anomaly_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_model:\n",
    "    model_file_name = f\"{sensor}_{extractor._ms_resample}x{preprocess._packed_windows}\"\n",
    "    with open(OUTPUT_FILE.format(file_name=model_file_name, file_type=\"log\"), \"w+\") as f:\n",
    "        f.write(f\"Train: <data: {len(train_dataset.get_train())}>, Test:<data: {len(train_dataset.get_test())}>\\n\")\n",
    "        f.write(\n",
    "            f\"Anomalies: <data: {len(anomaly_dataset.get_train())}>, Test:<data: {len(anomaly_dataset.get_test())}>\\n\"\n",
    "        )\n",
    "        f.write(evaluate(autoencoder, threshold))\n",
    "        f.write(evaluate(autoencoder, anomaly_threshold))\n",
    "    autoencoder.save_model(model_file_name)\n",
    "    print(\"[+] Model saved\")\n",
    "new_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "job_cancel_str = \"scancel \" + os.environ[\"SLURM_JOBID\"]\n",
    "os.system(job_cancel_str)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "TF training",
   "language": "python",
   "name": "train_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
